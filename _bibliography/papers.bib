---
---
@inproceedings{vud,
  title={Variational Uncertainty Decomposition for In-Context Learning},
  author={Jayasekera*, I. Shavindra and Si*, Jacob and Valdettaro, Filippo and Chen, Wenlong and Faisal, Aldo and Li, Yingzhen},
  booktitle={the 39th Conference on Neural Information Processing System, 2025.},
  abstract={As large language models (LLMs) gain popularity in conducting prediction tasks in-context, understanding the sources of uncertainty in in-context learning becomes essential to ensuring reliability. The recent hypothesis of in-context learning performing predictive Bayesian inference opens the avenue for Bayesian uncertainty estimation, particularly for decomposing uncertainty into epistemic uncertainty due to lack of in-context data and aleatoric uncertainty inherent in the in-context prediction task. However, the decomposition idea remains under-explored due to the intractability of the latent parameter posterior from the underlying Bayesian model. In this work, we introduce a variational uncertainty decomposition framework for in-context learning without explicitly sampling from the latent parameter posterior, by optimising auxiliary inputs as probes to obtain an upper bound to the aleatoric uncertainty of an LLM's in-context learning procedure. Through experiments on synthetic and real-world tasks, we show quantitatively and qualitatively that the decomposed uncertainties obtained from our method exhibit desirable properties of epistemic and aleatoric uncertainty.},
  year={2025},
  abbr={NeurIPS},
  pdf={https://arxiv.org/abs/2509.02327},
  code={https://github.com/jacobyhsi/VUD},
  selected={true}
}

@inproceedings{si2024interpretabnet,
  title={InterpreTabNet: Distilling Predictive Signals from Tabular Data by Salient Feature Interpretation},
  author={Si, Jacob and Cheng*, Wendy Yusi and Cooper*, Michael and Krishnan, Rahul},
  booktitle={the 41st International Conference on Machine Learning, 2024.},
  abstract={Tabular data are omnipresent in various sectors of industries. Neural networks for tabular data such as TabNet have been proposed to make predictions while leveraging the attention mechanism for interpretability. However, the inferred attention masks are often dense, making it challenging to come up with rationales about the predictive signal. To remedy this, we propose InterpreTabNet, a variant of the TabNet model that models the attention mechanism as a latent variable sampled from a Gumbel-Softmax distribution. This enables us to regularize the model to learn distinct concepts in the attention masks via a KL Divergence regularizer. It prevents overlapping feature selection by promoting sparsity which maximizes the model's efficacy and improves interpretability to determine the important features when predicting the outcome. To assist in the interpretation of feature interdependencies from our model, we employ a large language model (GPT-4) and use prompt engineering to map from the learned feature mask onto natural language text describing the learned signal. Through comprehensive experiments on real-world datasets, we demonstrate that InterpreTabNet outperforms previous methods for interpreting tabular data while attaining competitive accuracy.},
  year={2024},
  abbr={ICML},
  honor={Spotlight Presentation [top 3.5%]},
  award={Spotlight},
  pdf={https://arxiv.org/abs/2406.00426},
  code={https://github.com/jacobyhsi/InterpreTabNet},
  selected={true}
}

@inproceedings{si2025tabrep,
  title={TabRep: Training Tabular Diffusion Models with a Simple and Effective Continuous Representation},
  author={Si, Jacob and Ou*, Zijing and Qu*, Mike and Xiang*, Zhengrui and Li, Yingzhen},
  booktitle={the 42nd International Conference on Machine Learning, Foundation Models for Structured Data Workshop, 2025.},
  abstract={Diffusion models have been the predominant generative model for tabular data generation. However, they face the conundrum of modeling under a separate versus a unified data representation. The former encounters the challenge of jointly modeling all multi-modal distributions of tabular data in one model. While the latter alleviates this by learning a single representation for all features, it currently leverages sparse suboptimal encoding heuristics and necessitates additional computation costs. In this work, we address the latter by presenting TabRep, a tabular diffusion architecture trained with a unified continuous representation. To motivate the design of our representation, we provide geometric insights into how the data manifold affects diffusion models. The key attributes of our representation are composed of its density, flexibility to provide ample separability for nominal features, and ability to preserve intrinsic relationships. Ultimately, TabRep provides a simple yet effective approach for training tabular diffusion models under a continuous data manifold. Our results showcase that TabRep achieves superior performance across a broad suite of evaluations. It is the first to synthesize tabular data that exceeds the downstream quality of the original datasets while preserving privacy and remaining computationally efficient.},
  year={2025},
  abbr={ICML FMSD},
  pdf={https://arxiv.org/abs/2504.04798},
  code={https://github.com/jacobyhsi/TabRep},
  selected={true}
}

@inproceedings{tabrag,
  title={TabRAG: Tabular Document Retrieval via Structured Language Representations},
  author={Si*, Jacob and Qu*, Mike and Lee, Michelle and Li, Yingzhen},
  booktitle={Submission.},
  abstract={Ingesting data for Retrieval-Augmented Generation (RAG) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting TabRAG, a parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations. TabRAG outperforms existing popular parsing-based methods for generation and retrieval.},
  year={2025},
  abbr={Under Review},
  selected={true}
}

@incollection{si2022assessing,
  title={Assessing Infant Mortality Rate: Problems stemming from Household Living Conditions, Women’s Education and Health},
  author={Si, Jacob and Alexander, Rohan},
  booktitle={"Telling Stories with Data: With Applications in R" by Rohan Alexander},
  abstract={What areas can be improved in order to promote the well-being of women in India and hence, reduce the infant mortality rate? Utilizing the data from the 1998-1999 India National Family Health Survey provided by the Demographic and Health Survey (DHS) program, we look to depict the demographics of Indian women and infants in different states of India. We have found that the root causes of poor infant mortality rates stem from having poor living conditions that affect the likelihood of women to attain education and understand the importance of antenatal care and birth delivery assistance. We also explore other factors such as potentially inheritable traits (unhealthy body weight and anaemia disease) as well as an infant’s diet. These factors are crucial in the development of an infant and the reduction of the infant mortality rate.},
  year={2022},
  abbr={Book Chapter},
  publisher={CRC Press},
  pdf={https://tellingstorieswithdata.com/inputs/pdfs/paper-4-2022-jacob_yoke_hong_si.pdf},
  selected={true}
}
